
## Kubernetes Architecture Diagram

![image](https://github.com/user-attachments/assets/e451f2e5-689d-4fbb-a558-e8662f57f5e7)

### Nodes

- The main component of the Kubernetes architecture is a resource called as a **Node**.
- Nodes are individual computers or VMs.

### Cluster

-  When these Nodes are connected together, you will get a Kubernetes cluster.
-  Example, if you connect 3 VMs (Nodes), you will have a Kubernetes cluster with 3 nodes.
-  Every workload that you deploy to the cluster runs on a particular node.
-  This node is either auto-selected by one of the Kubernetes components called the **kube-scheduler**, or you can manually define some rules on which node the workload should run. 

### Two Types of Nodes

#### 1. Control Plane Node

   -  This is the "brain" of the cluster.

   - It makes all the important decisions — like where to run apps, checking health, managing updates, etc.

   - It holds all the critical parts that help Kubernetes work.

   - If this node goes down and there's only one — the whole system could stop working.That’s why, in real-life systems (like big websites or businesses), people use more than one Control Plane  — so that if one goes down, the others keep everything running.

   - This setup is called High Availability (HA) — it makes your Kubernetes system more reliable and safe.

#### 2. Worker Node

   - These are the "employees" or "workers".

   - The worker node (or minion) as it is also known, is were the containers are hosted.

   - They actually run your apps.

   - They listen to the Control Plane for instructions.
   - Worker nodes can be scaled up or down depending on how much traffic or load your application has.

## Control Plane Components 

#### 1.API Server

- The front door of the Kubernetes cluster.

- It receives all requests (from users or other parts of Kubernetes(via kubectl or other tools))

- It checks who’s making the request

- It makes sure that the request is safe and allowed (authentication & authorization)

- Then it forwards the request to the correct part of the cluster

#### 2.ETCD

- IT's is like the brain or memory of your Kubernetes cluster.

- It stores all the important information about your cluster, like: What’s running?, What’s been created or deleted?, What’s the current status of different parts of the system?

- Only the API Server is allowed to talk directly to ETCD.If any other part of Kubernetes wants to read or update information in etcd, it must go through the API Server.

-  High Availability ETCD cluster (more than one etcd instance)

- There are two main ways to set up etcd in a cluster:

   1. Stacked etcd

       - etcd runs inside the Control Plane

       - This is the most common setup

       -  Easy to manage for most clusters

   2. External etcd

       - etcd runs outside the Kubernetes cluster (like on a different machine)

        - Kubernetes talks to it by using its IP address or URL, which is set in the API Server’s settings
 
#### 3. Scheduler

- Pod

  - In Kubernetes, containers don’t run by themselves — they run inside something called a pod.

  - A pod can hold one or more containers (apps).

  - Pods run on nodes.(When you ask Kubernetes to run something, it picks a node and puts the pod there.)

- Who Chooses the Node?

   That job is done by a component called the **kube-scheduler**.

   - It looks at all the available nodes

   - Checks which one is the best fit (based on memory, CPU, etc.)

   - Then places your pod on that node

   - You can either:

     -  kube-scheduler can decide automatically, or

     - Set rules to guide where your pod should go (like picking a node with certain labels)

#### 4.Controller Manager

- controllers are like watchdogs that keep an eye on the cluster. They check if things are running the way they should, and if not, they take action to fix it.

 - There are different controllers for different jobs, like:

    - Deployment Controller – Makes sure the right number of pods are always running.

    - Replication Controller – Keeps a specific number of copies (replicas) of a pod running.

     - Endpoint Controller – Connects services to the right pods.

    - Namespace Controller – Manages namespaces (groupings of resources).

    - Service Account Controller – Manages service accounts for pods.

 - All these controllers are part of a bigger component called the **Controller Manager**, which runs all the controllers in one place.

 - Also can create our own custom controller.

#### 5.Cloud Controller Manager
- Cloud Controller Manager (CCM) is like a bridge between Kubernetes and cloud provider to create or remove resources (like VMs or networking stuff)

- Example:If cluster has 10 nodes, and they’re all full. You want to run more apps, but there’s no space. The CCM can ask your cloud provider to create a new VM (node) and add it to the cluster — automatically.

## Worker Nodes Components

#### 1.Kubelet

  -  Kubelet runs on every node (as a system service, not inside a pod).

  -  It listens for instructions from the Control Plane and makes sure the right pods are running on the node.

  -  Without the Kubelet, no pods can be created on that node.So only **kubelet creates a container**.

#### 2.Kube Proxy

- Kube-Proxy is like the network traffic manager for Kubernetes cluster.
- It makes sure that all parts of the cluster can talk to each other.

- It routes traffic between services and pods.

- It supports common network types like TCP, UDP, and SCTP.
- Kube-Proxy is installed on every node in the cluster.
- It keeps a routing table that helps it know where to send traffic inside the cluster.

#### 3.Container Runtime

- Kubernetes is a tool that runs and manages containers, but it doesn’t create containers by itself. It uses a special software called a container runtime to do that.

- The Kubelet tells the node: "I need this pod with these containers."

- The container runtime is what actually creates and **runs those containers inside the pod**.

## Namespace

- In Kubernetes, a Namespace is like a folder or a room inside the cluster.

- It helps to organize and separate our resources (like pods, services, deployments) so that they don’t all live in the same space.

**Uses:**
- To separate environments (like dev, test, and production)

- To avoid name conflicts (you can have a web-app in dev and another web-app in prod)

- To assign different permissions for different teams or projects

- To limit resources (like CPU or memory) per namespace

## Install Kubernetes locally using minicube

### Step-by-Step Setup:

#### 1.Install kubectl on Linux

Step 1: Download the latest release
```
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
```
Step 2: Make the file executable
```
chmod +x kubectl
```
Step 3: Move the binary to your path
```
sudo mv kubectl /usr/local/bin/
```
Step 4: Verify installation
```
kubectl version --client
```
![image](https://github.com/user-attachments/assets/4f88d611-910d-4ff9-b16d-1eb4a2ca44f8)

#### 2.Install Minikube

```
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube
```
#### 3.Start Minikube:
```
minikube start
```
![image](https://github.com/user-attachments/assets/76452a7f-ac63-43b6-a775-58313695d42a)

#### 4.Check if your cluster is running:
```
kubectl get nodes
```
![image](https://github.com/user-attachments/assets/62e39ead-a394-4a88-aa0d-2d233f774b56)






