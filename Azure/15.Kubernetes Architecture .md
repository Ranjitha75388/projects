### Nodes

- The main component of the Kubernetes architecture is a resource called as a **Node**.
- Nodes are individual computers or VMs.

### Cluster

-  When these Nodes are connected together, you will get a Kubernetes cluster.
-  Example, if you connect 3 VMs (Nodes), you will have a Kubernetes cluster with 3 nodes.
-  Every workload that you deploy to the cluster runs on a particular node.
-  This node is either auto-selected by one of the Kubernetes components called the **kube-scheduler**, or you can manually define some rules on which node the workload should run. 

### Two Types of Nodes

#### 1. Control Plane Node

   -  This is the "brain" of the cluster.

   - It makes all the important decisions — like where to run apps, checking health, managing updates, etc.

   - It holds all the critical parts that help Kubernetes work.

   - If this node goes down and there's only one — the whole system could stop working.That’s why, in real-life systems (like big websites or businesses), people use more than one Control Plane  — so that if one goes down, the others keep everything running.

   - This setup is called High Availability (HA) — it makes your Kubernetes system more reliable and safe.

#### 2. Worker Node

   - These are the "employees" or "workers".

   - The worker node (or minion) as it is also known, is were the containers are hosted.

   - They actually run your apps.

   - They listen to the Control Plane for instructions.
   - Worker nodes can be scaled up or down depending on how much traffic or load your application has.

### Control Plane Components 

#### 1.API Server

- The front door of the Kubernetes cluster.

- It receives all requests (from users or other parts of Kubernetes(via kubectl or other tools))

- It checks who’s making the request

- It makes sure that the request is safe and allowed (authentication & authorization)

- Then it forwards the request to the correct part of the cluster

#### 2.ETCD

- IT's is like the brain or memory of your Kubernetes cluster.

- It stores all the important information about your cluster, like: What’s running?, What’s been created or deleted?, What’s the current status of different parts of the system?

- Only the API Server is allowed to talk directly to ETCD.If any other part of Kubernetes wants to read or update information in etcd, it must go through the API Server.

-  High Availability ETCD cluster (more than one etcd instance)

- There are two main ways to set up etcd in a cluster:

   1. Stacked etcd

       - etcd runs inside the Control Plane

       - This is the most common setup

       -  Easy to manage for most clusters

   2. External etcd

       - etcd runs outside the Kubernetes cluster (like on a different machine)

        - Kubernetes talks to it by using its IP address or URL, which is set in the API Server’s settings
 
#### 3. Scheduler

- Pod

  - In Kubernetes, containers don’t run by themselves — they run inside something called a pod.

  - A pod can hold one or more containers (apps).

  - Pods run on nodes.(When you ask Kubernetes to run something, it picks a node and puts the pod there.)

- Who Chooses the Node?

   That job is done by a component called the **kube-scheduler**.

   - It looks at all the available nodes

   - Checks which one is the best fit (based on memory, CPU, etc.)

   - Then places your pod on that node

   - You can either:

     -  kube-scheduler can decide automatically, or

     - Set rules to guide where your pod should go (like picking a node with certain labels)

#### 4.Controller Manager

- controllers are like watchdogs that keep an eye on the cluster. They check if things are running the way they should, and if not, they take action to fix it.

 - There are different controllers for different jobs, like:

    - Deployment Controller – Makes sure the right number of pods are always running.

    - Replication Controller – Keeps a specific number of copies (replicas) of a pod running.

     - Endpoint Controller – Connects services to the right pods.

    - Namespace Controller – Manages namespaces (groupings of resources).

    - Service Account Controller – Manages service accounts for pods.

 - All these controllers are part of a bigger component called the **Controller Manager**, which runs all the controllers in one place.

 - Also can create our own custom controller.

#### 5.Cloud Controller Manager

- Cloud Controller Manager (CCM) is like a bridge between Kubernetes and cloud provider to create or remove resources (like VMs or networking stuff)

- Example:If cluster has 10 nodes, and they’re all full. You want to run more apps, but there’s no space. The CCM can ask your cloud provider to create a new VM (node) and add it to the cluster — automatically.














